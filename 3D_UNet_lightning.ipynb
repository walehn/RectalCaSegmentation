{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitpytorchconda31c7a5eb31904fc0a44985442d0e7944",
   "display_name": "Python 3.8.5 64-bit ('pytorch': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "from random import *\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import time, math\n",
    "\n",
    "import torchio as tio\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.data import CacheDataset, DataLoader, partition_dataset\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.losses import DiceLoss, GeneralizedDiceLoss\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import compute_meandice\n",
    "from monai.networks.nets import UNet\n",
    "from monai.transforms import (\n",
    "    AsDiscrete, Compose, LoadNiftid, ToTensord, AddChanneld, LabelToContour,\n",
    ")\n",
    "from monai.utils import set_determinism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MONAI_DATA_DIRECTORY\"] = \"./data\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0, 1, 2, 3'\n",
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyperparameter setting\n",
    "set_determinism(seed=0)\n",
    "\n",
    "bs = 4\n",
    "Height = 144\n",
    "Width = 144\n",
    "Depth = 16\n",
    "epoch_num = 10\n",
    "l_rate = 1e-3\n",
    "multi_GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(pl.LightningModule):\n",
    "    def __init__(self,bs,Height,Width,Depth,epoch_num,l_rate, multi_GPU):\n",
    "        super().__init__()\n",
    "        self._model = UNet(\n",
    "            dimensions=3,\n",
    "            in_channels=1,\n",
    "            out_channels=2,\n",
    "            channels=(32, 64, 128, 256, 512),\n",
    "            strides=(2, 2, 2, 2),\n",
    "            num_res_units=3,\n",
    "            norm=Norm.BATCH,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        self.loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "        self.post_pred = AsDiscrete(argmax=True, to_onehot=True, n_classes=2)\n",
    "        self.post_label = AsDiscrete(to_onehot=True, n_classes=2)\n",
    "        self.best_val_metric = 0\n",
    "        self.best_val_epoch = 0\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def foward(self, x):\n",
    "        return self._model(x)\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        data_dir = os.path.join(root_dir, \"nifti_data\")\n",
    "        train_images = sorted(glob.glob(os.path.join(data_dir, \"image\", \"*.nii.gz\")))\n",
    "        train_labels = sorted(glob.glob(os.path.join(data_dir, \"mask\", \"*.nii.gz\")))\n",
    "        data_dicts = [\n",
    "            {\"image\": image_name, \"label\": label_name}\n",
    "            for image_name, label_name in zip(train_images, train_labels)\n",
    "        ]\n",
    "\n",
    "        ### image augmentation transform with monai and torchio API\n",
    "\n",
    "        # HistogramStandardization parameter calculation\n",
    "        #histogram_landmarks_path = 'landmarks.npy'\n",
    "        # landmarks = tio.HistogramStandardization.train(\n",
    "        #     train_images,\n",
    "        #     output_path=histogram_landmarks_path,\n",
    "        # )\n",
    "        # np.set_printoptions(suppress=True, precision=3)\n",
    "        self.train_data, self.val_data, self.test_data = partition_dataset(data_dicts, ratios = [0.8, 0.1, 0.1], shuffle = True)\n",
    "        print('\\n'+'Training set:', len(self.train_data), 'subjects')\n",
    "        print('Validation set:', len(self.val_data), 'subjects')\n",
    "        print('Test set:', len(self.test_data), 'subjects')\n",
    "  \n",
    "    def train_dataloader(self):\n",
    "        # transform setting\n",
    "        train_transforms_monai = [\n",
    "                LoadNiftid(keys=[\"image\", \"label\"]),\n",
    "                AddChanneld(keys=[\"image\", \"label\"]),\n",
    "                ToTensord(keys=[\"image\", \"label\"]),\n",
    "        ]\n",
    "\n",
    "        train_transforms_io = [\n",
    "                tio.CropOrPad((Height, Width, Depth),mask_name='label', include=[\"image\", \"label\"]),\n",
    "                #tio.HistogramStandardization({'image': landmarks}, include=[\"image\"]),\n",
    "                tio.ZNormalization(masking_method=tio.ZNormalization.mean, include=[\"image\"]),\n",
    "                tio.RandomNoise(p=0.1, include=[\"image\"]),\n",
    "                tio.RandomFlip(axes=(0,), include=[\"image\", \"label\"]),\n",
    "        ]\n",
    "        train_transforms = Compose(train_transforms_monai + train_transforms_io)\n",
    "        \n",
    "        train_ds = CacheDataset(data=self.train_data, transform=train_transforms, cache_rate=1.0, num_workers=4)\n",
    "        train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=4)\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        validation_transforms_monai = [\n",
    "                LoadNiftid(keys=[\"image\", \"label\"]),\n",
    "                AddChanneld(keys=[\"image\", \"label\"]),\n",
    "                ToTensord(keys=[\"image\", \"label\"]),\n",
    "        ]\n",
    "\n",
    "        validation_transforms_io = [\n",
    "            tio.CropOrPad((Height, Width, Depth), include=[\"image\", \"label\"], mask_name='label'),\n",
    "            # tio.HistogramStandardization({'image': landmarks}, include=[\"image\"]),\n",
    "            tio.ZNormalization(masking_method=tio.ZNormalization.mean, include=[\"image\"]),\n",
    "        ]\n",
    "        val_transforms = Compose(validation_transforms_monai + validation_transforms_io)\n",
    "        val_ds = CacheDataset(data=self.val_data, transform=val_transforms, cache_rate=1.0, num_workers=4)\n",
    "        val_loader = DataLoader(val_ds, batch_size=4, num_workers=4)\n",
    "        return val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_transforms_monai = [\n",
    "                LoadNiftid(keys=[\"image\", \"label\"]),\n",
    "                AddChanneld(keys=[\"image\", \"label\"]),\n",
    "                ToTensord(keys=[\"image\", \"label\"]),\n",
    "        ]\n",
    "        test_transforms_io = [\n",
    "            tio.CropOrPad((Height, Width, Depth), include=[\"image\", \"label\"], mask_name='label'),\n",
    "            # tio.HistogramStandardization({'image': landmarks}, include=[\"image\"]),\n",
    "            tio.ZNormalization(masking_method=tio.ZNormalization.mean, include=[\"image\"]),\n",
    "        ]\n",
    "        test_transforms = Compose(test_transforms_monai + test_transforms_io)\n",
    "        test_ds = CacheDataset(data=self.test_data,transform=test_transforms, cache_rate=1.0, num_workers=4)\n",
    "        test_loader = DataLoader(test_ds, batch_size=1, num_workers=4)\n",
    "        return test_loader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self._model.parameters(), lr = l_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "            images, labels = batch[\"image\"], batch[\"label\"]\n",
    "            output = self.foward(images)\n",
    "            loss = self.loss_function(output, labels)\n",
    "            tensorboard_logs = {\"train_loss\": loss.item()}\n",
    "            return {\"loss\":loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        roi_size = (Height, Width, Depth)\n",
    "        sw_batch_size = 1\n",
    "        outputs = sliding_window_inference(images, roi_size, sw_batch_size, self.foward)\n",
    "        loss = self.loss_function(outputs, labels)\n",
    "        outputs = self.post_pred(outputs)\n",
    "        labels = self.post_label(labels)\n",
    "        metric = compute_meandice(y_pred=outputs, y=labels, include_background=False)\n",
    "        return {\"val_loss\": loss, \"val_metric\": metric}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        val_metric, val_loss, num_items = 0, 0, 0\n",
    "        for output in outputs:\n",
    "            val_metric += output[\"val_metric\"].sum().item()\n",
    "            val_loss += output[\"val_loss\"].sum().item()\n",
    "            num_items += len(output[\"val_metric\"])\n",
    "        mean_val_metric = torch.tensor(val_metric/num_items)\n",
    "        mean_val_loss = torch.tensor(val_loss / num_items)\n",
    "        tensorboard_logs = {\"val_metric\":mean_val_metric, \"val_loss\":mean_val_loss}\n",
    "        if mean_val_metric>self.best_val_metric:\n",
    "            self.best_val_metric = mean_val_metric\n",
    "            self.best_val_epoch = self.current_epoch + 1\n",
    "        print(\n",
    "            f\"\\ncurrent epoch: {self.current_epoch} current mean metric: {mean_val_metric:.4f}\"\n",
    "            f\"\\nbest mean metric: {self.best_val_metric:.4f} at epoch: {self.best_val_epoch}\"\n",
    "        )\n",
    "        return {\"log\": tensorboard_logs}\n",
    "\n",
    "    def test_step(self,batch,batch_idx):\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        roi_size = (Height, Width, Depth)\n",
    "        sw_batch_size = 1\n",
    "        test_image = images\n",
    "        test_output = sliding_window_inference(test_image, roi_size, sw_batch_size, self.foward)\n",
    "        # plot the slice [:, :, rand]\n",
    "        j = randint(0, len(test_image[0,0,0,0,:])-1)\n",
    "        plt.figure(\"check\", (20, 4))\n",
    "\n",
    "        plt.subplot(1, 5, 1)\n",
    "        plt.title(f\"original image {batch_idx}\")\n",
    "        plt.imshow(test_image.detach().cpu()[0, 0, :, :, j], cmap=\"gray\")\n",
    "\n",
    "        plt.subplot(1, 5, 2)\n",
    "        plt.title(f\"Ground truth mask\")\n",
    "        plt.imshow(labels.detach().cpu()[0, 0, :, :, j])\n",
    "\n",
    "        plt.subplot(1, 5, 3)\n",
    "        plt.title(f\"AI predicted mask\")\n",
    "        argmax = AsDiscrete(argmax=True)(test_output)\n",
    "        plt.imshow(argmax.detach().cpu()[0, 0, :, :, j])\n",
    "\n",
    "        plt.subplot(1, 5, 4)\n",
    "        plt.title(f\"contour\")\n",
    "        contour = LabelToContour()(argmax)\n",
    "        plt.imshow(contour.detach().cpu()[0, 0, :, :, j])\n",
    "\n",
    "        plt.subplot(1, 5, 5)\n",
    "        plt.title(f\"overaying predicted\")\n",
    "        map_image = test_image.clone().detach()\n",
    "        map_image[argmax==1] = map_image.max()\n",
    "        plt.imshow(map_image.detach().cpu()[0, 0, :, :, j], cmap=\"gray\")\n",
    "        plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(bs, Height, Width, Depth, epoch_num, l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup logger and checkpoints\n",
    "chk_path = \"./checkpoints\"\n",
    "log_dir = os.path.join(chk_path,\"logs\")\n",
    "tb_logger = pl.loggers.TensorBoardLogger(save_dir=log_dir)\n",
    "checkpoint_callback = pl.callbacks.model_checkpoint.ModelCheckpoint(\n",
    "    filepath=os.path.join(log_dir, \"{epoch}-{val_loss:.2f}-{val_metric:.2f}\")\n",
    ")\n",
    "\n",
    "#inti pl trainer\n",
    "trainer = pl.Trainer(\n",
    "    gpus=4,\n",
    "    max_epochs=epoch_num,\n",
    "    logger=tb_logger,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    num_sanity_val_steps=1,\n",
    "    auto_lr_find=False,\n",
    "    accelerator='dp', num_nodes=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "trainer.fit(net)\n",
    "print(f\"train completed, best_metric: {net.best_val_metric:.4f} at epoch: {net.best_val_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}